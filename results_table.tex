\begin{longtable}{lllllllllrrr}
\toprule
dataset & layers & epochs & dr\_mha & drop\_mha & dr\_mlp & dr\_ratio & lambda\_dr & gamma & train\_loss & test\_loss & test\_map \\
\midrule
V3 & 2 & 10 & V & none & 2 & 0.2 & 0.001 & 0.05 & 0.1750 & 0.5452 & 0.6552 \\
V3 & 2 & 10 & V & none & 1 & 0.2 & 0.001 & 0.05 & 0.1750 & 0.5452 & 0.6552 \\
V3 & 2 & 5 & V & none & 1 & 0.2 & 0.001 & 0.01 & 0.1906 & 0.5335 & 0.6551 \\
V3 & 2 & 10 & V & none & 1 & 0.2 & 0.001 & 0.1 & 0.1633 & 0.5659 & 0.6517 \\
V3 & 2 & 10 & V & none & 2 & 0.2 & 0.001 & 0.1 & 0.1633 & 0.5659 & 0.6517 \\
V3 & 2 & 5 & - & none & 1 & 0.2 & 0.001 & 0.01 & 0.1424 & 0.5732 & 0.6485 \\
V3 & 2 & 5 & - & none & 2 & 0.2 & 0.001 & 0.01 & 0.1424 & 0.5732 & 0.6485 \\
V3 & 2 & 10 & - & out & 0 & 0.2 & - & 0.05 & 0.1233 & 0.6089 & 0.6480 \\
V3 & 2 & 5 & - & out & 0 & 0.2 & - & 0.05 & 0.1234 & 0.6089 & 0.6480 \\
V3 & 2 & 5 & - & out & 1 & 0.2 & - & 0.01 & 0.1463 & 0.5823 & 0.6477 \\
V3 & 2 & 10 & - & - & 1 & 0.2 & - & 0.05 & 0.1335 & 0.6018 & 0.6472 \\
V3 & 2 & 10 & - & none & 2 & 0.2 & 0.001 & 0.05 & 0.1198 & 0.6073 & 0.6453 \\
V3 & 2 & 10 & - & none & 1 & 0.2 & 0.001 & 0.05 & 0.1198 & 0.6073 & 0.6453 \\
V3 & 2 & 10 & - & - & 1 & 0.2 & - & 0.1 & 0.1196 & 0.6332 & 0.6440 \\
V3 & 2 & 10 & - & - & 0 & 0.2 & - & 0.05 & 0.1368 & 0.6058 & 0.6439 \\
V3 & 2 & 10 & - & out & 1 & 0.2 & - & 0.05 & 0.1243 & 0.6130 & 0.6438 \\
V3 & 2 & 10 & - & out & 0 & 0.2 & - & 0.1 & 0.1090 & 0.6464 & 0.6403 \\
V3 & 2 & 10 & - & - & 0 & 0.2 & - & 0.1 & 0.1232 & 0.6363 & 0.6387 \\
V3 & 2 & 10 & - & none & 2 & 0.2 & 0.001 & 0.1 & 0.1048 & 0.6495 & 0.6370 \\
V3 & 2 & 10 & - & none & 1 & 0.2 & 0.001 & 0.1 & 0.1048 & 0.6495 & 0.6370 \\
V3 & 2 & 10 & - & out & 1 & 0.2 & - & 0.1 & 0.1098 & 0.6547 & 0.6351 \\
V3 & 2 & 5 & V & none & 2 & 0.2 & 0.001 & 0.5 & 0.1482 & 0.6565 & 0.6261 \\
V3 & 2 & 10 & - & attn & 0 & 0.2 & - & 0.05 & 0.1825 & 0.6007 & 0.6240 \\
V3 & 2 & 10 & - & attn & 0 & 0.2 & - & 0.1 & 0.1707 & 0.6207 & 0.6231 \\
V3 & 2 & 10 & - & attn & 1 & 0.2 & - & 0.1 & 0.1739 & 0.6171 & 0.6204 \\
V3 & 2 & 10 & - & attn & 1 & 0.2 & - & 0.05 & 0.1864 & 0.5971 & 0.6200 \\
V3 & 2 & 5 & Q & none & 1 & 0.2 & 0.001 & 0.01 & 0.2231 & 0.5799 & 0.6135 \\
V3 & 2 & 5 & Q & none & 2 & 0.2 & 0.001 & 0.01 & 0.2231 & 0.5799 & 0.6135 \\
V3 & 2 & 10 & Q & none & 1 & 0.2 & 0.001 & 0.05 & 0.2041 & 0.6013 & 0.6130 \\
V3 & 2 & 10 & Q & none & 2 & 0.2 & 0.001 & 0.05 & 0.2041 & 0.6013 & 0.6130 \\
V3 & 2 & 5 & K & none & 2 & 0.2 & 0.001 & 0.01 & 0.2236 & 0.5771 & 0.6119 \\
V3 & 2 & 5 & K & none & 1 & 0.2 & 0.001 & 0.01 & 0.2236 & 0.5771 & 0.6119 \\
V3 & 2 & 10 & K & none & 2 & 0.2 & 0.001 & 0.05 & 0.2046 & 0.5959 & 0.6117 \\
V3 & 2 & 10 & K & none & 1 & 0.2 & 0.001 & 0.05 & 0.2046 & 0.5959 & 0.6117 \\
V3 & 2 & 10 & Q & none & 2 & 0.2 & 0.001 & 0.1 & 0.1920 & 0.6221 & 0.6116 \\
V3 & 2 & 10 & Q & none & 1 & 0.2 & 0.001 & 0.1 & 0.1920 & 0.6221 & 0.6116 \\
V3 & 2 & 10 & K & none & 2 & 0.2 & 0.001 & 0.1 & 0.1925 & 0.6172 & 0.6111 \\
V3 & 2 & 10 & K & none & 1 & 0.2 & 0.001 & 0.1 & 0.1925 & 0.6172 & 0.6111 \\
V3 & 2 & 5 & - & - & 0 & 0.2 & - & 0.5 & 0.0865 & 0.7718 & 0.6008 \\
V3 & 2 & 5 & - & attn & 1 & 0.2 & - & 0.5 & 0.1418 & 0.7081 & 0.6005 \\
V3 & 2 & 5 & - & attn & 0 & 0.2 & - & 0.5 & 0.1379 & 0.7245 & 0.5968 \\
Anet2016\_feature\_v2 & 2 & 10 & V & none & 2 & 0.2 & 0.001 & 0.05 & 0.1944 & 0.6651 & 0.5690 \\
Anet2016\_feature\_v2 & 2 & 10 & V & none & 1 & 0.2 & 0.001 & 0.05 & 0.1944 & 0.6651 & 0.5690 \\
Anet2016\_feature\_v2 & 2 & 10 & V & none & 1 & 0.2 & 0.001 & 0.1 & 0.1812 & 0.6899 & 0.5682 \\
Anet2016\_feature\_v2 & 2 & 10 & V & none & 2 & 0.2 & 0.001 & 0.1 & 0.1812 & 0.6899 & 0.5682 \\
Anet2016\_feature\_v2 & 2 & 10 & V & none & 2 & 0.2 & 0.001 & 0.01 & 0.2140 & 0.6348 & 0.5675 \\
Anet2016\_feature\_v2 & 2 & 10 & V & none & 1 & 0.2 & 0.001 & 0.01 & 0.2140 & 0.6348 & 0.5675 \\
Anet2016\_feature\_v2 & 2 & 10 & - & - & 1 & 0.2 & - & 0.05 & 0.1434 & 0.7236 & 0.5608 \\
Anet2016\_feature\_v2 & 2 & 10 & - & - & 0 & 0.2 & - & 0.05 & 0.1454 & 0.7233 & 0.5576 \\
Anet2016\_feature\_v2 & 2 & 10 & - & - & 1 & 0.2 & - & 0.1 & 0.1290 & 0.7659 & 0.5564 \\
Anet2016\_feature\_v2 & 2 & 10 & - & - & 1 & 0.2 & - & 0.01 & 0.1668 & 0.6858 & 0.5556 \\
Anet2016\_feature\_v2 & 2 & 10 & - & - & 0 & 0.2 & - & 0.1 & 0.1320 & 0.7620 & 0.5544 \\
Anet2016\_feature\_v2 & 2 & 10 & - & out & 1 & 0.2 & - & 0.05 & 0.1253 & 0.7770 & 0.5519 \\
Anet2016\_feature\_v2 & 2 & 10 & - & - & 0 & 0.2 & - & 0.01 & 0.1671 & 0.6956 & 0.5514 \\
Anet2016\_feature\_v2 & 2 & 10 & - & out & 1 & 0.2 & - & 0.01 & 0.1485 & 0.7247 & 0.5507 \\
Anet2016\_feature\_v2 & 2 & 10 & - & out & 0 & 0.2 & - & 0.01 & 0.1489 & 0.7145 & 0.5481 \\
Anet2016\_feature\_v2 & 2 & 10 & - & out & 1 & 0.2 & - & 0.1 & 0.1103 & 0.8210 & 0.5471 \\
Anet2016\_feature\_v2 & 2 & 10 & - & out & 0 & 0.2 & - & 0.05 & 0.1268 & 0.7628 & 0.5463 \\
Anet2016\_feature\_v2 & 2 & 10 & - & none & 2 & 0.2 & 0.001 & 0.05 & 0.1237 & 0.7471 & 0.5463 \\
Anet2016\_feature\_v2 & 2 & 10 & - & attn & 0 & 0.2 & - & 0.05 & 0.2027 & 0.6964 & 0.5445 \\
Anet2016\_feature\_v2 & 2 & 10 & - & attn & 0 & 0.2 & - & 0.1 & 0.1896 & 0.7214 & 0.5437 \\
Anet2016\_feature\_v2 & 2 & 10 & - & attn & 1 & 0.2 & - & 0.1 & 0.1850 & 0.7247 & 0.5427 \\
Anet2016\_feature\_v2 & 2 & 10 & - & attn & 1 & 0.2 & - & 0.05 & 0.1983 & 0.6997 & 0.5417 \\
Anet2016\_feature\_v2 & 2 & 10 & - & out & 0 & 0.2 & - & 0.1 & 0.1123 & 0.8078 & 0.5404 \\
Anet2016\_feature\_v2 & 2 & 10 & - & none & 2 & 0.2 & 0.001 & 0.1 & 0.1079 & 0.7916 & 0.5390 \\
Anet2016\_feature\_v2 & 2 & 10 & - & attn & 0 & 0.2 & - & 0.01 & 0.2232 & 0.6770 & 0.5363 \\
Anet2016\_feature\_v2 & 2 & 10 & - & attn & 1 & 0.2 & - & 0.01 & 0.2185 & 0.6867 & 0.5318 \\
- & 3 & - & - & - & - & - & - & - & 0.1420 & 0.7250 & - \\
- & 3 & - & - & 1/log & - & - & - & - & 0.1314 & 0.7498 & - \\
\bottomrule
\end{longtable}
